Prediction Assignment Writeup
Setting seed, loading libraries and dataset

set.seed(123)
require(data.table);require(ggplot2);require(caret);require(randomForest)
## Warning: package 'data.table' was built under R version 3.5.2
## Warning: package 'ggplot2' was built under R version 3.5.2
## Warning: package 'caret' was built under R version 3.5.2
## Warning: package 'randomForest' was built under R version 3.5.2
pmltrain <- read.csv("pml-training.csv")
pmltest <- read.csv("pml-testing.csv")
First I am doing a quick exploration of the data to see if there is something in the data that will affect my modelling decisions.

Checking which column names are common among testing and training, so we can exclude the ones who are not common. Checking the class balance in the training set to see whether there is anything in particular we should be concerned with. Plotting the classe variable against the first 5 (example exploratory plot).

length(intersect(colnames(pmltrain),colnames(pmltest)))
## [1] 159
barplot(table(pmltrain$classe))


splom(classe~pmltrain[1:5], data = pmltrain)


159 variables in common, everyone except classe, and the target variable is fairly even blanances across the different classes.

Inspecting if there are some features that only include NAs in the testing set - these should not be used in model training, because they cannot help the prediction in the test set.

test_na <-sapply(pmltest, FUN=function(x){
  length(which(is.na(x)))
})
length(names(test_na[test_na==20]))
## [1] 100
There are 100 features that are only NAs, removing these from the training set (and test set). Splitting pmltrain into training and test (validation) set and removing NA features. Making the split with the same class balace as “classe” - the target variable.

inTrain <- createDataPartition(pmltrain$classe, p = 0.7, list=F)
training <- pmltrain[inTrain,!names(pmltrain) %in% names(test_na[test_na>0])]
testing <- pmltrain[-inTrain,!names(pmltrain) %in% names(test_na[test_na>0])]
Controlling the class balance

prop.table(table(training$classe))
## 
##         A         B         C         D         E 
## 0.2843416 0.1934920 0.1744195 0.1639368 0.1838101
prop.table(table(testing$classe))
## 
##         A         B         C         D         E 
## 0.2844520 0.1935429 0.1743415 0.1638063 0.1838573
Checking if the split generates any all-NA features in either split

table(sapply(training, function(x){
  all(is.na(x))
}))
## 
## FALSE 
##    60
table(sapply(testing, function(x){
  all(is.na(x))
}))
## 
## FALSE 
##    60
No ALL-NA features in either of the splits. Preforming an LDA as part of the exploration as a benchmark and to see if anything is fishy.

fitlda <- train(classe~., method="lda", data = training)
confusionMatrix(fitlda)
## Bootstrapped (25 reps) Confusion Matrix 
## 
## (entries are percentual average cell counts across resamples)
##  
##           Reference
## Prediction    A    B    C    D    E
##          A 28.5  0.0  0.0  0.0  0.0
##          B  0.0 19.4  0.0  0.0  0.0
##          C  0.0  0.0 17.4  0.0  0.0
##          D  0.0  0.0  0.0 16.3  0.0
##          E  0.0  0.0  0.0  0.0 18.4
##                             
##  Accuracy (average) : 0.9999
The model fits perfectly, which is suspicious. Checking variable importance for potensial leak of the target variable.

lda_varimp <- varImp(fitlda)
head(lda_varimp$importance, 5)
##                                A          B          C          D
## X                    100.0000000 100.000000 100.000000 100.000000
## user_name              4.8927679   4.892768   4.892768   5.128175
## raw_timestamp_part_1  25.7032925  25.703293  25.703293  25.703293
## raw_timestamp_part_2   0.8218359   1.562432   2.216072   1.487203
## cvtd_timestamp        11.9371229  15.418413  11.017269  12.525417
##                               E
## X                    100.000000
## user_name              2.525818
## raw_timestamp_part_1  21.587376
## raw_timestamp_part_2   1.562432
## cvtd_timestamp        15.418413
Variable X seems to perfectly predict variable classe

qplot(X, magnet_forearm_z, colour=classe, data = training)


This is confirmed by the plot, variable X is ordered/sorted by the target variable. I don’t expect the testing set to have this property.

Dropping the X variable (feature) in testing and training. Saving test case for pmltest in another variable for using in reporting predictions.

cases <- pmltest$X
pmltrain$X <- NULL
pmltest$X <- NULL
training$X <- NULL
testing$X <- NULL
Fitting models
Doing 10-fold cross validation, 1 time (1 repetition). Reevaluating this setting after evaluation of cross validation accuracy and variability.

fitControl <- trainControl(method="cv", number=10, repeats=1)
## Warning: `repeats` has no meaning for this resampling method.
Fitting three models and comparing: Decision tree, linear discriminant analysis and gradient boosting.

fitTree <- train(classe~., method="rpart", data = training, trControl=fitControl)
fitlda <- train(classe~., method="lda", data = training, trControl=fitControl)
fitgbm <- train(classe~., method="gbm", data = training, verbose=F, trControl=fitControl)
Inspecting cross validation accuracy (mean) and variability (standard deviation)

print("Decision tree mean and standard deviation:",mean(fitTree$resample$Accuracy))
## [1] "Decision tree mean and standard deviation:"
mean(fitTree$resample$Accuracy)
## [1] 0.6053579
sd(fitTree$resample$Accuracy)
## [1] 0.05497847
print("LDA mean and standard deviation:")
## [1] "LDA mean and standard deviation:"
mean(fitlda$resample$Accuracy)
## [1] 0.8527379
sd(fitlda$resample$Accuracy)
## [1] 0.01094222
print("GBM mean and standard deviation:")
## [1] "GBM mean and standard deviation:"
mean(fitgbm$resample$Accuracy)
## [1] 0.996579
sd(fitgbm$resample$Accuracy)
## [1] 0.002143283
Summary of the results - the GBM model has the highest mean accuracy and lowest standard deviation, - the lda model also has a decent accuracy and a bit higher standard deviation than gbm - decision tree model preforms the worst and has the highest standard deviation.

Checking prediction accuracy on my own testing/validation set. I am expecting similar accuracy as the mean from the cross validation.

Alternatively the expected out of sample error (cv error) is 1 minus the accuracy. Expected out of sample errors for the respective models:

1-mean(fitTree$resample$Accuracy)
## [1] 0.3946421
1-mean(fitlda$resample$Accuracy)
## [1] 0.1472621
1-mean(fitgbm$resample$Accuracy)
## [1] 0.00342099
predTree <- predict(fitTree,testing)
predlda <- predict(fitlda, testing)
predgbm <- predict(fitgbm, testing)
confusionMatrix(predTree, testing$classe)$table
##           Reference
## Prediction    A    B    C    D    E
##          A 1299  318   27   49   12
##          B    4  166    0    0    0
##          C  160  182  862  319  228
##          D  204  473  137  596  361
##          E    7    0    0    0  481
confusionMatrix(predTree, testing$classe)$overall[1] # test set accuracy
##  Accuracy 
## 0.5784197
confusionMatrix(predlda, testing$classe)$table
##           Reference
## Prediction    A    B    C    D    E
##          A 1521  136    3    0    0
##          B  130  835  112    2    0
##          C   23  157  885  116    3
##          D    0   11   23  793   97
##          E    0    0    3   53  982
confusionMatrix(predlda, testing$classe)$overall[1] # test set accuracy
##  Accuracy 
## 0.8523364
confusionMatrix(predgbm, testing$classe)$table
##           Reference
## Prediction    A    B    C    D    E
##          A 1673    0    0    0    0
##          B    1 1136    3    0    0
##          C    0    2 1020    2    0
##          D    0    1    3  960    3
##          E    0    0    0    2 1079
confusionMatrix(predgbm, testing$classe)$overall[1] # test set accuracy
##  Accuracy 
## 0.9971113
All three models preforms as expected, the deviation from the cross validation accuracy is low and I do not see a reason to change resampling method or adding repetitons. LDA seems superior to rpart tree model, but gbm is best in terms of accuracy. Choosing to predict on pmltest with the gbm model. Checking if there is anything to gain from increasing the number of boosting iterations.

plot(fitgbm)


print(fitgbm$bestTune)
##   n.trees interaction.depth shrinkage n.minobsinnode
## 9     150                 3       0.1             10
Accuracy has plateaued, and further tuning would only yield decimal gain. - The best tuning parameters was 150 trees (boosting iterations), - interaction depth 3 - shrinkage 0.1.

Deciding to predict with this model.

preds <- predict(fitgbm, pmltest)
data.frame(cases, preds)
##    cases preds
## 1      1     B
## 2      2     A
## 3      3     B
## 4      4     A
## 5      5     A
## 6      6     E
## 7      7     D
## 8      8     B
## 9      9     A
## 10    10     A
## 11    11     B
## 12    12     C
## 13    13     B
## 14    14     A
## 15    15     E
## 16    16     E
## 17    17     A
## 18    18     B
## 19    19     B
## 20    20     B
cat("Predictions: ", paste(predict(fitgbm, pmltest)))
## Predictions:  B A B A A E D B A A B C B A E E A B B B
